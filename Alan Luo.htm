<!DOCTYPE HTML>
<!-- saved from url=(0020)https://alan.vision/ -->
<!DOCTYPE html PUBLIC "" ""><HTML lang="en"><HEAD><META content="IE=11.0000" 
http-equiv="X-UA-Compatible">
   
<META charset="utf-8">   
<META http-equiv="X-UA-Compatible" content="IE=edge">   
<META name="viewport" content="width=device-width, initial-scale=1">   
<META name="description" content="">   
<META name="author" content="">   <TITLE>Alan Luo</TITLE>   <!-- Latest compiled and minified CSS --> 
  <LINK href="Alan%20Luo_files/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous" 
integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"> 
  <!-- Custom CSS -->   <LINK href="Alan%20Luo_files/style.css" 
rel="stylesheet"> 
<META name="GENERATOR" content="MSHTML 11.00.10570.1001"></HEAD> <!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap
scrollspy function --> 
<BODY id="page-top" data-target=".navbar-fixed-top" data-spy="scroll"><!-- Navigation --> 
<NAV class="navbar navbar-default navbar-fixed-top" role="navigation">
<DIV class="container">
<DIV class="navbar-header page-scroll"><BUTTON class="navbar-toggle" type="button" 
data-toggle="collapse" data-target=".navbar-ex1-collapse"><SPAN 
class="sr-only">Toggle navigation</SPAN>         <SPAN 
class="icon-bar"></SPAN><SPAN class="icon-bar"></SPAN><SPAN 
class="icon-bar"></SPAN></BUTTON>       <A class="navbar-brand page-scroll" 
href="https://alan.vision/#page-top">Alan Luo</A>     </DIV><!-- Collect the nav links, forms, and other content for toggling --> 
    
<DIV class="collapse navbar-collapse navbar-ex1-collapse">
<UL class="nav navbar-nav"><!-- Hidden li included to remove active class from about link when scrolled up past about section --> 
          
  <LI class="hidden"><A class="page-scroll" 
  href="https://alan.vision/#page-top"></A></LI>
  <LI><A class="page-scroll" href="https://alan.vision/#about">About</A></LI>
  <LI><A class="page-scroll" 
  href="https://alan.vision/#publication">Publications</A></LI></UL></DIV><!-- /.navbar-collapse --> 
  </DIV><!-- /.container --> </NAV><!-- Section: Introduction --> <SECTION 
class="section-intro" id="intro">
<DIV class="container-fluid">
<DIV class="row">
<DIV class="col-xs-6 col-xs-offset-3 my-info"><A href="https://youtu.be/dQw4w9WgXcQ"></A>
<H1><A href="https://youtu.be/dQw4w9WgXcQ">Zelun (Alan) Luo</A></H1>
<P>          PhD Student<BR><A href="http://svl.stanford.edu/">Stanford Vision 
and Learning Lab</A><BR>          Advisor: <A href="https://profiles.stanford.edu/fei-fei-li">Prof. 
Fei-Fei Li</A><BR>          alanzluo at stanford dot edu<BR><A href="https://scholar.google.com/citations?user=MVUbYCkAAAAJ&amp;hl=en">[Google 
Scholar]</A>           <A href="https://github.com/d1ngn1gefe1">[Github]</A>     
      <A href="https://alan.vision/intro/cv.pdf">[CV (updated on July 2018)]</A> 
        </P></DIV></DIV></DIV></SECTION><!-- Section: About --> <SECTION class="section-about" 
id="about">
<DIV class="container-fluid">
<DIV class="row">
<H1 class="subtitle">About</H1></DIV>
<DIV class="row">
<P class="text-left">        I am a second-year PhD student in the Computer 
Science department at Stanford University. I am working in the <A href="http://vision.stanford.edu/">Stanford 
Vision and Learning Lab</A>, advised by <A href="http://vision.stanford.edu/feifeili/">Prof. 
Fei-Fei Li</A>.       </P>
<P class="text-left">        I am a member of the <A href="https://aicare.stanford.edu/">Stanford 
Program in AI-Assisted Care (PAC)</A>, which is a collaboration between the 
Stanford AI Lab and Stanford Clinical Excellence Research Center that aims to 
use computer vision and machine learning to create AI-assisted smart healthcare 
spaces.       </P>
<P class="text-left">        Before that, I received my master's degree from 
Stanford University and my bachelor's degree from the University of Illinois 
Urbana-Champaign. During my master's study, I worked in the <A href="http://vision.stanford.edu/">Stanford 
Vision and Learning Lab</A> with <A 
href="http://vision.stanford.edu/feifeili/">Prof. Fei-Fei Li</A>, and the <A 
href="http://med.stanford.edu/cerc.html">Clinical Excellence Research Center</A> 
with <A href="https://med.stanford.edu/profiles/arnold-milstein">Prof. Arnold 
Milstein</A>. During my undergraduate study, I spent three years reseaching in 
the <A href="http://light.ece.illinois.edu/">Quantitative Light Imaging 
Laboratory</A> with <A href="http://light.ece.illinois.edu/">Prof. Gabriel 
Popescu</A>, and the <A href="http://vision.ai.illinois.edu/">Computer Vision 
and Robotics Laboratory</A> with <A 
href="https://filebox.ece.vt.edu/~jbhuang/">Prof. Jia-Bin Huang</A> and <A href="http://vision.ai.illinois.edu/ahuja.html">Prof. 
Narendra Ahuja</A>.       </P></DIV>
<DIV class="row education">
<H3 class="text-left">Education</H3>
<DIV class="col-sm-4 col-xs-6 school"><IMG class="img-fluid school-fig" src="Alan%20Luo_files/stanford_seal.png"> 
        
<P>Stanford University</P>
<P><SMALL>Doctor of Philosophy</SMALL></P>
<P><SMALL>Computer Science</SMALL></P>
<P></P></DIV>
<DIV class="col-sm-4 col-xs-6 school"><IMG class="img-fluid school-fig" src="Alan%20Luo_files/stanford_seal.png"> 
        
<P>Stanford University</P>
<P><SMALL>Master of Science</SMALL></P>
<P><SMALL>Computer Science</SMALL></P>
<P></P></DIV>
<DIV class="col-sm-4 col-xs-6 school"><IMG class="img-fluid school-fig" src="Alan%20Luo_files/uiuc_seal.png"> 
        
<P>University of Illinois Urbana-Champaign</P>
<P><SMALL>Bachelor of Science</SMALL></P>
<P><SMALL>Electrical and Computer Engineering</SMALL></P>
<P></P></DIV></DIV>
<DIV class="row industry">
<H3 class="text-left">Industry</H3>
<DIV class="col-sm-3 col-xs-6 company"><IMG class="img-fluid company-fig img-circle" 
src="Alan%20Luo_files/facebook.png">         
<P>Facebook Research<BR>
<P><SMALL>Research Intern</SMALL></P>
<P><SMALL>Summer 2019</SMALL></P>
<P></P></DIV>
<DIV class="col-sm-3 col-xs-6 company"><IMG class="img-fluid company-fig img-circle" 
src="Alan%20Luo_files/google.png">         
<P>Google Cloud AI<BR>
<P><SMALL>Research Intern</SMALL></P>
<P><SMALL>Summer 2017</SMALL></P>
<P></P></DIV>
<DIV class="col-sm-3 col-xs-6 company"><IMG class="img-fluid company-fig img-circle" 
src="Alan%20Luo_files/amazon.png">         
<P>Amazon A9<BR>
<P><SMALL>Research Intern</SMALL></P>
<P><SMALL>Summer 2016</SMALL></P>
<P></P></DIV>
<DIV class="col-sm-3 col-xs-6 company"><IMG class="img-fluid company-fig img-circle" 
src="Alan%20Luo_files/yahoo.png">         
<P>Yahoo<BR>
<P><SMALL>Software Engineering Intern</SMALL></P>
<P><SMALL>Summer 2015</SMALL></P>
<P></P></DIV></DIV>
<DIV class="row teaching">
<H3 class="text-left">Teaching</H3>
<UL>
  <LI>
  <P><A href="http://cs337.stanford.edu/">MED 277/CS 337 (AI-Assisted Health 
  Care)</A>: Head Teaching Assistant (Fall 2018); Teaching Assistant (Winter 
  2018)</P></LI>
  <LI>
  <P><A href="http://cs231n.stanford.edu/">CS 231N (Convolutional Neural 
  Networks for Visual Recognition)</A>: Teaching Assistant (Spring 
2017)</P></LI>
  <LI>
  <P><A href="http://cs224n.stanford.edu/">CS 224N (Natural Language Processing 
  with Deep Learning)</A>: Teaching Assistant (Winter 2017)</P></LI>
  <LI>
  <P><A href="http://cs109.stanford.edu/">CS 109 (Probability for Computer 
  Scientists)</A>: Teaching Assistant (Winter 2016 &amp; Spring 2016)</P></LI>
  <LI>
  <P><A href="http://cs131.stanford.edu/">CS 131 (Computer Vision: Foundations 
  and Applications)</A>: Teaching Assistant (Fall 2015); Head Teaching Assistant 
  (Fall 2016)</P></LI></UL></DIV>
<DIV class="row professional">
<H3 class="text-left">Professional Activities</H3>
<UL>
  <LI>
  <P>AI Conference Reviewer: CVPR 2020, AAAI 2020, NeurIPS 2019, ICCV 2019, ICML 
  2019, CVPR 2019, CVPR 2018</P></LI>
  <LI>
  <P>Medical Conference Reviewer: MLHC 2019, MLHC 2018</P></LI>
  <LI>
  <P>Journal Reviewer: TPAMI (Jun. 2019), TPAMI (Nov. 
2018)</P></LI></UL></DIV></DIV></SECTION><!-- Section: Publication--> <SECTION 
class="section-publication" id="publication">
<DIV class="container-fluid">
<DIV class="row">
<H1 class="subtitle">Selected Publications</H1></DIV>
<DIV class="row">
<H3 class="subfield-first">Computer Vision and Deep Learning</H3>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/luo2018graph.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Graph Distillation for Action Detection with Privileged Information</H4>
<P><STRONG>Zelun Luo</STRONG>, Jun-Ting Hsieh, Lu Jiang, Juan Carlos Niebles, Li 
Fei-Fei</P>
<P>European Conference on Computer Vision (ECCV) 2018</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="In this work, we propose a technique that tackles the video understanding problem under a realistic, demanding condition in which we have limited labeled data and partially observed training modalities. Common methods such as transfer learning do not take advantage of the rich information from extra modalities potentially available in the source domain dataset. On the other hand, previous work on cross-modality learning only focuses on a single domain or task. In this work, we propose a graph-based distillation method that incorporates rich privileged information from a large multi-modal dataset in the source domain, and shows an improved performance in the target domain where data is scarce. Leveraging both a large-scale dataset and its extra modalities, our method learns a better model for temporal action detection and action classification without needing to have access to these modalities during test time. We evaluate our approach on action classification and temporal action detection tasks, and show that our models achieve the state-of-the-art performance on the PKU-MMD and NTU RGB+D datasets." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="https://arxiv.org/abs/1712.00108" target="_blank">PDF</A>             <A 
class="btn btn-default btn-xs" href="http://alan.vision/eccv18_graph" target="_blank">Project</A> 
            <A class="btn btn-default btn-xs" href="http://alan.vision/publications/luo2018graph_poster.pdf" 
target="_blank">Poster</A>             <A class="btn btn-default btn-xs" href="https://github.com/google/graph_distillation" 
target="_blank">Code</A>           </DIV></DIV></DIV>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" onerror="this.onerror=null;this.src='publications/zou2018dfnet.gif';" 
src="Alan%20Luo_files/zou2018dfnet.gif"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Network 
Consistency</H4>
<P>Yuliang Zou, <STRONG>Zelun Luo</STRONG>, Jia-Bin Huang</P>
<P>European Conference on Computer Vision (ECCV) 2018</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="We present an unsupervised learning framework for simultaneously training single-view depth prediction and optical flow estimation models using unlabeled video sequences. Existing unsupervised methods often exploit brightness constancy and spatial smoothness priors to train depth or flow models. In this paper, we propose to leverage geometric consistency as additional supervisory signals. Our core idea is that for rigid regions we can use the predicted scene depth and camera motion to synthesize 2D optical flow by backprojecting the induced 3D scene flow. The discrepancy between the rigid flow (from depth prediction and camera motion) and the estimated flow (from optical flow model) allows us to impose a cross-task consistency loss. While all the networks are jointly optimized during training, they can be applied independently at test time. Extensive experiments demonstrate that our depth and flow models compare favorably with state-of-the-art unsupervised methods." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="https://arxiv.org/abs/1809.01649" target="_blank">PDF</A>             <A 
class="btn btn-default btn-xs" href="http://yuliang.vision/DF-Net/" target="_blank">Project</A> 
            <A class="btn btn-default btn-xs" href="https://github.com/vt-vl-lab/DF-Net" 
target="_blank">Code</A>           </DIV></DIV></DIV>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/luo2017label.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Label Efficient Learning of Transferable Representations across Domains and 
Tasks</H4>
<P><STRONG>Zelun Luo</STRONG>, Yuliang Zou, Judy Hoffman, Li Fei-Fei</P>
<P>Conference on Neural Information Processing Systems (NIPS) 2017</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="We propose a framework that learns a representation transferable across different domains and tasks in a data efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="https://arxiv.org/abs/1712.00123" target="_blank">PDF</A>             <A 
class="btn btn-default btn-xs" href="http://alan.vision/nips17_label/poster.pdf" 
target="_blank">Poster</A>             <A class="btn btn-default btn-xs" href="http://alan.vision/nips17_label/" 
target="_blank">Project</A>           </DIV></DIV></DIV>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/luo2017unsupervised.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Unsupervised Learning of Long-Term Motion Dynamics for Videos</H4>
<P><STRONG>Zelun Luo</STRONG>, Boya Peng, De-An Huang, Alexandre Alahi, Li 
Fei-Fei</P>
<P>Conference on Computer Vision and Pattern Recognition (CVPR) 2017</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="We present an unsupervised representation learning approach&#10;                                              that compactly encodes the motion dependencies&#10;                                              in videos. Given a pair of images from a video clip, our&#10;                                              framework learns to predict the long-term 3D motions. To&#10;                                              reduce the complexity of the learning framework, we propose&#10;                                              to describe the motion as a sequence of atomic 3D&#10;                                              flows computed with RGB-D modality. We use a Recurrent&#10;                                              Neural Network based Encoder-Decoder framework&#10;                                              to predict these sequences of flows. We argue that in order&#10;                                              for the decoder to reconstruct these sequences, the encoder&#10;                                              must learn a robust video representation that captures&#10;                                              long-term motion dependencies and spatial-temporal relations.&#10;                                              We demonstrate the effectiveness of our learned temporal&#10;                                              representations on activity classification across multiple&#10;                                              modalities and datasets such as NTU RGB+D and MSR&#10;                                              Daily Activity 3D. Our framework is generic to any input&#10;                                              modality, i.e., RGB, depth, and RGB-D videos." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="https://arxiv.org/pdf/1701.01821.pdf" target="_blank">PDF</A>             
<A class="btn btn-default btn-xs" href="https://alan.vision/publications/CVPR2017-poster.png" 
target="_blank">Poster</A>           </DIV></DIV></DIV>
<DIV class="row paper paper-last">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" onerror="this.onerror=null;this.src='publications/haque2016towards.png';" 
src="Alan%20Luo_files/haque2016towards.gif"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Towards Viewpoint Invariant 3D Human Pose Estimation</H4>
<P>Albert Haque, <STRONG>Zelun Luo*</STRONG>, Boya Peng*, Alexandre Alahi, 
Serena Yeung, Li Fei-Fei</P>
<P>European Conference on Computer Vision (ECCV) 2016</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="We propose a viewpoint invariant model for 3D human pose&#10;                                              estimation from a single depth image. To achieve this, our discriminative&#10;                                              model embeds local regions into a learned viewpoint invariant feature&#10;                                              space. Formulated as a multi-task learning problem, our model is able to&#10;                                              selectively predict partial poses in the presence of noise and occlusion.&#10;                                              Our approach leverages a convolutional and recurrent network architecture&#10;                                              with a top-down error feedback mechanism to self-correct previous&#10;                                              pose estimates in an end-to-end manner. We evaluate our model on a&#10;                                              previously published depth dataset and a newly collected human pose&#10;                                              dataset containing 100K annotated depth images from extreme viewpoints.&#10;                                              Experiments show that our model achieves competitive performance&#10;                                              on frontal views while achieving state-of-the-art performance on&#10;                                              alternate viewpoints." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="https://arxiv.org/pdf/1603.07076.pdf" target="_blank">PDF</A>             
<A class="btn btn-default btn-xs" href="https://www.alberthaque.com/projects/viewpoint_3d_pose/" 
target="_blank">Website</A>           </DIV></DIV></DIV></DIV>
<DIV class="row">
<H3 class="subfield">AI-Assisted Healthcare</H3>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/darke2018vision.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Vision-Based Gait Analysis for Senior Care</H4>
<P>Evan Darke, Anin Sayana, Kelly Shen, David Xue, Jun-Ting Hsieh, <STRONG>Zelun 
Luo</STRONG>, Li-Jia Li, N, Lance Downing, Arnold Milstein, Li Fei-Fei</P>
<P>ML4H: Machine Learning for Health, NeurIPS 2018, Montreal, Canada, December 
8, 2018</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="As the senior population rapidly increases, it is challenging yet crucial to provide effective long-term care for seniors who live at home or in senior care facilities. Smart senior homes, which have gained widespread interest in the healthcare community, have been proposed to improve the well-being of seniors living independently. In particular, non-intrusive, cost-effective sensors placed in these senior homes enable gait characterization, which can provide clinically relevant information including mobility level and early neurodegenerative disease risk. In this paper, we present a method to perform gait analysis from a single camera placed within the home. We show that we can accurately calculate various gait parameters, demonstrating the potential for our system to monitor the long-term gait of seniors and thus aid clinicians in understanding a patient’s medical profile." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="https://arxiv.org/pdf/1812.00169.pdf" target="_blank">PDF</A>           
</DIV></DIV></DIV>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" onerror="this.onerror=null;this.src='publications/luo2018computer.gif';" 
src="Alan%20Luo_files/luo2018computer.gif"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Computer Vision-based Descriptive Analytics of Seniors' Daily Activities for 
Long-term Health Monitoring</H4>
<P><STRONG>Zelun Luo*</STRONG>, Jun-Ting Hsieh*, Niranjan Balachandar, Serena 
Yeung, Guido Pusiol, Jay Luxenberg, Grace Li, Li-Jia Li, N. Lance Downing, 
Arnold Milstein, Li Fei-Fei</P>
<P>Machine Learning for Healthcare (MLHC) 2018, Stanford, CA, August 17-18, 
2018</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="One in twenty-five patients admitted to a hospital will suffer from a hospital acquired infection. If we can intelligently track healthcare staff, patients, and visitors, we can better understand the sources of such infections. We envision a smart hospital capable of increasing operational efficiency and improving patient care with less spending. In this paper, we propose a non-intrusive vision-based system for tracking people’s activity in hospitals. We evaluate our method for the problem of measuring hand hygiene compliance. Empirically, our method outperforms existing solutions such as proximity-based techniques and covert in-person observational studies. We present intuitive, qualitative results that analyze human movement patterns and conduct spatial analytics which convey our method’s interpretability. This work is a first step towards a computer-vision based smart hospital and demonstrates promising results for reducing hospital acquired infections." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/5b7373254ae23704e284bdf4/1534292778467/18.pdf" 
target="_blank">PDF</A>           </DIV></DIV></DIV>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/haque2017towards.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Towards Vision-Based Smart Hospitals: A System for Tracking and Monitoring 
Hand Hygiene Compliance</H4>
<P>Albert Haque, Michelle Guo, Alexandre Alahi, Serena Yeung, <STRONG>Zelun 
Luo</STRONG>, Alisha Rege, Amit Singh, Jeffrey Jopling, N. Lance Downing, 
William Beninati, Terry Platchek, Arnold Milstein, Li Fei-Fei</P>
<P>Machine Learning for Healthcare (MLHC) 2017, Boston, MA, August 18-19, 
2017</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="Nations around the world face rising demand for costly long-term care for seniors. Patterns&#10;                                              in seniors' activities of daily living, such as sleeping, sitting, standing, walking, etc. can&#10;                                              provide caregivers useful clues regarding seniors' health. As the senior population continues&#10;                                              to grow worldwide, continuous manual monitoring of seniors' daily activities will&#10;                                              become more and more challenging for caregivers. Thus to improve caregivers' ability&#10;                                              to assist seniors, an automated system for monitoring and analyzing patterns in seniors&#10;                                              activities of daily living would be useful. A possible approach to implementing such a&#10;                                              system involves wearable sensors, but this approach is intrusive and requires adherence by&#10;                                              patients. In this paper, using a dataset we collected from an assisted-living facility for&#10;                                              seniors, we present a novel computer vision-based approach that leverages nonintrusive,&#10;                                              privacy-compliant multi-modal sensors and state-of-the-art computer vision techniques for&#10;                                              continuous activity detection to remotely detect and provide long-term descriptive analytics&#10;                                              of senior activities. These analytics include both qualitative and quantitative descriptions&#10;                                              of senior daily activity patterns that can be interpreted by caregivers. Our work is progress&#10;                                              towards a smart senior home that uses computer vision to support caregivers in senior&#10;                                              healthcare to help meet the challenges of an aging worldwide population." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="https://arxiv.org/pdf/1708.00163.pdf" target="_blank">PDF</A>           
</DIV></DIV></DIV>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/luo2017computer.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Computer Vision-based Approach to Maintain Independent Living for 
Seniors</H4>
<P><STRONG>Zelun Luo</STRONG>, Alisha Rege, Guido Pusiol, Arnold Milstein, Li 
Fei-Fei, N. Lance Downing</P>
<P>American Medical Informatics Association (AMIA), Washington, DC, November 
4-8, 2017</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="Recent progress in developing cost-effective sensors and machine learning techniques has enabled new AI-assisted solutions for human behavior understanding. In this work, we investigate the use of thermal and depth sensors for the detection of daily activities, lifestyle patterns, emotions, and vital signs, as well as the development of intelligent mechanisms for accurate situational assessment and rapid response. We demonstrate an integrated solution for remote monitoring, assessment, and support of seniors living independently at home." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="https://alan.vision/publications/AMIA-Poster.pdf" 
target="_blank">Poster</A>             <A class="btn btn-default btn-xs" href="https://alan.vision/publications/luo2017computer.pdf" 
target="_blank">Manuscript</A>           </DIV></DIV></DIV>
<DIV class="row paper paper-last">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/yeung2015vision.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Vision-Based Hand Hygiene Monitoring in Hospitals</H4>
<P>Serena Yeung, Alexandre Alahi, <STRONG>Zelun Luo</STRONG>, Boya Peng, Albert 
Haque, Amit Singh, Terry Platchek,            Arnold Milstein, Li Fei-Fei</P>
<P>American Medical Informatics Association (AMIA), Chicago, November 12-16, 
2016</P>
<P>NIPS Workshop on Machine Learning for Healthcare, 2015</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="Recent progress in developing cost-effective depth sensors has enabled new AI-assisted&#10;                                              solutions such as assisted driving vehicles and smart spaces. Machine&#10;                                              learning techniques have been successfully applied on these depth signals to perceive&#10;                                              meaningful information about human behavior. In this work, we propose&#10;                                              to deploy depth sensors in hospital settings and use computer vision methods to&#10;                                              enable AI-assisted care. We aim to reduce visually-identifiable human errors such&#10;                                              as hand hygiene compliance, one of the leading causes of Health Care-Associated&#10;                                              Infection (HCAI) in hospitals." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="http://ai.stanford.edu/~syyeung/resources/vision_hand_hh_nipsmlhc.pdf" 
target="_blank">PDF</A>           </DIV></DIV></DIV></DIV>
<DIV class="row">
<H3 class="subfield">Biomedical Imaging and Diagnosis</H3>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/kandel2017label.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Label-Free Tissue Scanner for Colorectal Cancer Screening</H4>
<P>Mikhail E. Kandel, Shamira Sridharan, Jon Liang, <STRONG>Zelun Luo</STRONG>, 
Kevin Han, Virgilia Macias, Anish Shah,            Roshan Patel, Krishnarao 
Tangella, Andre Kajdacsy-Balla, Grace Guzman, Gabriel Popescu</P>
<P>Journal of Biomedical Optics, Opt. 22(6), 2017</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="The current practice of surgical pathology relies on external contrast&#10;                                              agents to reveal tissue architecture, which is then qualitatively examined&#10;                                              by a trained pathologist. The diagnosis is based on the comparison with&#10;                                              standardized empirical, qualitative assessments of limited objectivity.&#10;                                              We propose an approach to pathology based on interferometric imaging of&#10;                                              “unstained” biopsies, which provides unique capabilities for quantitative&#10;                                              diagnosis and automation. We developed a label-free tissue scanner based&#10;                                              on “quantitative phase imaging,” which maps out optical path length at&#10;                                              each point in the field of view and, thus, yields images that are sensitive&#10;                                              to the “nanoscale” tissue architecture. Unlike analysis of stained tissue,&#10;                                              which is qualitative in nature and affected by color balance, staining&#10;                                              strength and imaging conditions, optical path length measurements are&#10;                                              intrinsically quantitative, i.e., images can be compared across different&#10;                                              instruments and clinical sites. These critical features allow us to automate&#10;                                              the diagnosis process. We paired our interferometric optical system with highly&#10;                                              parallelized, dedicated software algorithms for data acquisition, allowing us&#10;                                              to image at a throughput comparable to that of commercial tissue scanners while&#10;                                              maintaining the nanoscale sensitivity to morphology. Based on the measured phase&#10;                                              information, we implemented software tools for autofocusing during imaging, as&#10;                                              well as image archiving and data access. To illustrate the potential of our&#10;                                              technology for large volume pathology screening, we established an “intrinsic&#10;                                              marker” for colorectal disease that detects tissue with dysplasia or colorectal&#10;                                              cancer and flags specific areas for further examination, potentially improving&#10;                                              the efficiency of existing pathology workflows. " 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="https://www.spiedigitallibrary.org/journals/Journal_of_Biomedical_Optics/volume-22/issue-6/066016/Label-free-tissue-scanner-for-colorectal-cancer-screening/10.1117/1.JBO.22.6.066016.full" 
target="_blank">PDF &amp; Website</A>           </DIV></DIV></DIV>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/majeed2016towards.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Towards Quantitative Automated Histopathology of Breast Cancer using Spatial 
Light Interference Microscopy (SLIM)</H4>
<P>Hassaan Majeed, Tan H Nguyen, Mikhail E Kandel, Kevin Han, <STRONG>Zelun 
Luo</STRONG>, Virgilia Macias,            Krishnarao Tangella, Andre Balla, Minh 
N Do, Gabriel Popescu</P>
<P>United States and Canadian Academy of Pathology (USCAP), Seattle, WA, March 
12-18, 2016</P></DIV></DIV>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/majeed2015breast.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Breast Cancer Diagnosis using Spatial Light Interference Microscopy</H4>
<P>Hassaan Majeed, Mikhail E Kandel, Kevin Han, <STRONG>Zelun Luo</STRONG>, 
Virgilia Macias, Krishnarao Tangella,            Andre Balla, Gabriel 
Popescu</P>
<P>Journal of Biomedical Optics, Opt. 20(11), 2015</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="The standard practice in histopathology of breast cancers is to examine a hematoxylin and eosin (H&amp;E) stained tissue biopsy under a microscope to diagnose whether a lesion is benign or malignant. This determination is made based on a manual, qualitative inspection, making it subject to investigator bias and resulting in low throughput. Hence, a quantitative, label-free, and high-throughput diagnosis method is highly desirable. We present here preliminary results showing the potential of quantitative phase imaging for breast cancer screening and help with differential diagnosis. We generated phase maps of unstained breast tissue biopsies using spatial light interference microscopy (SLIM). As a first step toward quantitative diagnosis based on SLIM, we carried out a qualitative evaluation of our label-free images. These images were shown to two pathologists who classified each case as either benign or malignant. This diagnosis was then compared against the diagnosis of the two pathologists on corresponding H&amp;E stained tissue images and the number of agreements were counted. The agreement between SLIM and H&amp;E based diagnosis was 88% for the first pathologist and 87% for the second. Our results demonstrate the potential and promise of SLIM for quantitative, label-free, and high-throughput diagnosis." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="http://light.ece.illinois.edu/wp-content/uploads/2015/10/Hassaan_JBO_20_11_111210.pdf" 
target="_blank">PDF</A>             <A class="btn btn-default btn-xs" href="http://biomedicaloptics.spiedigitallibrary.org/article.aspx?articleid=2430724" 
target="_blank">Website</A>           </DIV></DIV></DIV>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/majeed2015high.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>High Throughput Imaging of Blood Smears using White Light Diffraction Phase 
Microscopy</H4>
<P>Hassaan Majeed, Mikhail E Kandel, Basanta Bhaduri, Kevin Han, <STRONG>Zelun 
Luo</STRONG>, Krishnarao Tangella,            Gabriel Popescu</P>
<P>SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="While automated blood cell counters have made great progress in detecting abnormalities in blood, the lack of specificity for a particular disease, limited information on single cell morphology and intrinsic uncertainly due to high throughput in these instruments often necessitates detailed inspection in the form of a peripheral blood smear. Such tests are relatively time consuming and frequently rely on medical professionals tally counting specific cell types. These assays rely on the contrast generated by chemical stains, with the signal intensity strongly related to staining and preparation techniques, frustrating machine learning algorithms that require consistent quantities to denote the features in question. Instead we opt to use quantitative phase imaging, understanding that the resulting image is entirely due to the structure (intrinsic contrast) rather than the complex interplay of stain and sample. We present here our first steps to automate peripheral blood smear scanning, in particular a method to generate the quantitative phase image of an entire blood smear at high throughput using white light diffraction phase microscopy (wDPM), a single shot and common path interferometric imaging technique." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204110" 
target="_blank">              PDF &amp; Website</A>           </DIV></DIV></DIV>
<DIV class="row paper">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/majeed2015diagnosis.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>Diagnosis of Breast Cancer Biopsies using Quantitative Phase Imaging</H4>
<P>Hassaan Majeed, Mikhail E Kandel, Kevin Han, <STRONG>Zelun Luo</STRONG>, 
Virgilia Macias, Krishnarao Tangella,            Andre Balla, Gabriel 
Popescu</P>
<P>SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="The standard practice in the histopathology of breast cancers is to examine a hematoxylin and eosin (H&amp;E) stained tissue biopsy under a microscope. The pathologist looks at certain morphological features, visible under the stain, to diagnose whether a tumor is benign or malignant. This determination is made based on qualitative inspection making it subject to investigator bias. Furthermore, since this method requires a microscopic examination by the pathologist it suffers from low throughput. A quantitative, label-free and high throughput method for detection of these morphological features from images of tissue biopsies is, hence, highly desirable as it would assist the pathologist in making a quicker and more accurate diagnosis of cancers. We present here preliminary results showing the potential of using quantitative phase imaging for breast cancer screening and help with differential diagnosis. We generated optical path length maps of unstained breast tissue biopsies using Spatial Light Interference Microscopy (SLIM). As a first step towards diagnosis based on quantitative phase imaging, we carried out a qualitative evaluation of the imaging resolution and contrast of our label-free phase images. These images were shown to two pathologists who marked the tumors present in tissue as either benign or malignant. This diagnosis was then compared against the diagnosis of the two pathologists on H&amp;E stained tissue images and the number of agreements were counted. In our experiment, the agreement between SLIM and H&amp;E based diagnosis was measured to be 88%. Our preliminary results demonstrate the potential and promise of SLIM for a push in the future towards quantitative, label-free and high throughput diagnosis." 
data-placement="bottom" data-trigger="hover">Abstract</A>             <A class="btn btn-default btn-xs" 
href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204106" 
target="_blank">              PDF &amp; Website</A>           </DIV></DIV></DIV>
<DIV class="row paper paper-last">
<DIV class="col-sm-3 col-sm-offset-0 col-xs-offset-2 col-xs-8 paper-fig"><IMG 
class="img-responsive" src="Alan%20Luo_files/kandel2015cpp.png"></DIV>
<DIV class="col-sm-9 col-xs-12 paper-info">
<H4>C++ Software Integration for a High-throughput Phase Imaging Platform</H4>
<P>Mikhail E Kandel, <STRONG>Zelun Luo</STRONG>, Kevin Han, Gabriel Popescu</P>
<P>SPIE Photonics West: BiOS, San Francisco, CA, February 7-12, 2015</P>
<DIV><A tabindex="0" title="Abstract" class="btn btn-default btn-xs" role="button" 
data-toggle="popover" data-content="The multi-shot approach in SLIM requires reliable, synchronous, and parallel operation of three independent hardware devices – not meeting these challenges results in degraded phase and slow acquisition speeds, narrowing applications to holistic statements about complex phenomena. The relative youth of quantitative imaging and the lack of ready-made commercial hardware and tools further compounds the problem as Higher level programming languages result in inflexible, experiment specific instruments limited by ill-fitting computational modules, resulting in a palpable chasm between promised and realized hardware performance. Furthermore, general unfamiliarity with intricacies such as background calibration, objective lens attenuation, along with spatial light modular alignment, makes successful measurements difficult for the inattentive or uninitiated. This poses an immediate challenge for moving our techniques beyond the lab to biologically oriented collaborators and clinical practitioners.&#10;                <br />To meet these challenges, we present our new Quantitative Phase Imaging pipeline, with improved instrument performance, friendly user interface and robust data processing features, enabling us to acquire and catalog clinical datasets hundreds of gigapixels in size." 
data-placement="bottom" data-trigger="hover" data-html="True">Abstract</A>       
      <A class="btn btn-default btn-xs" href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2204094" 
target="_blank">              PDF &amp; Website</A>           
</DIV></DIV></DIV></DIV></DIV></SECTION><!--Google Analytics--> 
<SCRIPT>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function () {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
    a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-100690611-1', 'auto');
  ga('send', 'pageview');
</SCRIPT>
 <!-- jQuery --> 
<SCRIPT src=""></SCRIPT>
 
<SCRIPT src="Alan%20Luo_files/jquery.easing.min.js"></SCRIPT>
 <!-- Bootstrap Core JavaScript --> 
<SCRIPT src="Alan%20Luo_files/bootstrap.min.js" crossorigin="anonymous" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"></SCRIPT>
 <!-- Custom JavaScript --> 
<SCRIPT src="Alan%20Luo_files/effects.js"></SCRIPT>
 </BODY></HTML>
